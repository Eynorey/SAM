* * *

### SAM Initiative

###### Skill Access Manager

* * *

# Master Test Plan

###### Version 1.0

## Table of Contents

1.  [Introduction](#markdown-header-1.-Introduction)
    1.  [Purpose](# 11-purpose)
    2.  [Scope](# 12-scope)
    3.  [Scope](# 12-scope)
    4.  [Document Terminology and Acronyms](# 13-document-terminology-and-acronyms)
    5.  [References](# 14-references)
    6.  [Document Structure](# 15-structure)
2.  <a href="">Evaluation Mission and Test Motivation</a>
    1.  <a href="">Background</a>
    2.  <a href="">Evaluation Mission</a>
    3.  <a href="">Test Motivators</a>
3.  [Test Approach](# 2---test-approach)
    1.  <a href="">Unit Testing</a>
    2.  [User Interface Testing](# 22-user-interface-testing)
    3. <a href="">Functional Testing</a>
4.  [Deliverables](# 3---deliverables)
    1.  [Test Evaluation Summaries](# 31-reporting-on-test-coverage)
    2.  [Reporting on Test Coverage](# 32-incident-logs-and-change-requests)
    3.  [Perceived Quality Reports](# 33-additional-automated-functional-test-scripts)
5.  [Testing Workflow](# 4---testing-workflow)
6.  [Environmental Needs](# 5---environmental-needs)
7.  [Responsibilities, Staffing and Training Needs](# 6---responsibilities-staffing-and-training-needs)
8.  [Milestones](# 7---milestones)
9.  [Risks, Dependencies, Assumptions and Constraints](# 8---risks-dependencies-assumptions-and-constraints)

# Master Test Plan

## 1\. Introduction

## 1.1 Purpose

The purpose of the Iteration Test Plan is to gather all of the information necessary to plan and control the test effort for the Skill Access Manager. It describes the approach to testing the software, and is the top-level plan generated and used by managers to direct the test effort. This Test Plan for the Skill Access Manager supports the following objectives:

*   Identifies the items that should be targeted by the tests.
*   Identifies the motivation for and ideas behind the test areas to be covered.
*   Outlines the testing approach that will be used.
*   Identifies the required resources and provides an estimate of the test efforts.
*   Lists the deliverable elements of the test project.

### 1.2 Scope

The application is tested using three different methods, each of which has a different scope. Unit tests only concern a single class or method each. This ensures, that the implemented code works independently. We decided to exclude code from our tests that is developed and provided or generated by the JHipster Framework, such as the Maintenance Dashboard, since it is in their responsibility. This document shows the following types of testing:

*   Unit tests
*   User Interface Testing

### 1.3 Intended Audience

This test plan is intended for technically advanced readers. It does not describe the application and should only be read by active project developers that touch the testing area.

### 1.4 Document Terminology and Acronyms

Definitions, Acronyms and Abbreviations relevant for the interpretation of the present document can be found in the [Project Glossary](https://eynorey.visualstudio.com/SAM%20-%20Smartify%20The%20World/_apps/hub/agile-extensions.wiki.wiki#/wiki/ProjectGlossary).

### 1.5 References

| Title | Date | Publishing organization |
| --- | --- | --- |
| [Project Glossary](https://eynorey.visualstudio.com/SAM%20-%20Smartify%20The%20World/_apps/hub/agile-extensions.wiki.wiki#/wiki/ProjectGlossary) | 23/10/16 | SAM Initiative |
| [Project Blog](https://smartifytheworld.wordpress.com/) | 27/10/16 | SAM Initiative |
| [UC-Estimations](https://eynorey.visualstudio.com/SAM%20-%20Smartify%20The%20World/_apps/hub/ms-devlabs.wiki.wiki#/wiki/Time-estimation) | 06/04/17 | SAM Initiative |
| [UC Specification Register](https://eynorey.visualstudio.com/SAM%20-%20Smartify%20The%20World/_apps/hub/agile-extensions.wiki.wiki#/docs/UC-Specification-Register) | 11/11/16 | SAM Initiative |
| [UC Specification Login/Logout](https://eynorey.visualstudio.com/SAM%20-%20Smartify%20The%20World/_apps/hub/ms-devlabs.wiki.wiki?/docs#/docs/UC-Specification-Login-Logout) | 19/04/17 | SAM Initiative |
| [UC Specification Manage Course](https://eynorey.visualstudio.com/SAM%20-%20Smartify%20The%20World/_apps/hub/agile-extensions.wiki.wiki#/docs/UC-Specification-ManageCourse) | 11/11/16 | SAM Initiative |
| [UC Specification See available credits](https://eynorey.visualstudio.com/SAM%20-%20Smartify%20The%20World/_apps/hub/ms-devlabs.wiki.wiki?/docs#/docs/UC-Specification-See-available-credits) | 11/04/17 | SAM Initiative |
| [UC Specification Search available courses](https://eynorey.visualstudio.com/SAM%20-%20Smartify%20The%20World/_apps/hub/ms-devlabs.wiki.wiki?/docs#/docs/UC-Specification-Search-available-courses) | 11/04/17 | SAM Initiative |
| [UC Specification Book Course](https://eynorey.visualstudio.com/SAM%20-%20Smartify%20The%20World/_apps/hub/ms-devlabs.wiki.wiki?/docs#/docs/UC-Specification-Book-course) | 12/04/17 | SAM Initiative |
| [UC Specification See schedule](https://eynorey.visualstudio.com/SAM%20-%20Smartify%20The%20World/_apps/hub/ms-devlabs.wiki.wiki?/docs#/docs/UC-Specification-See-schedule) | 11/04/17 | SAM Initiative |
| [UC Specification Manage Account](https://eynorey.visualstudio.com/SAM%20-%20Smartify%20The%20World/_apps/hub/ms-devlabs.wiki.wiki?/docs#/docs/UC-Specification-Manage-account) | 13/04/17 | SAM Initiative |
| [Software Requirements Specification](https://eynorey.visualstudio.com/SAM%20-%20Smartify%20The%20World/_apps/hub/ms-devlabs.wiki.wiki?/docs#/docs/SRS) | 11/11/16 | SAM Initiative |
| [Software-Architecture-Document](https://eynorey.visualstudio.com/SAM%20-%20Smartify%20The%20World/_apps/hub/ms-devlabs.wiki.wiki?/docs#/docs/Software-Architecture-Document) | 24/11/16 | SAM Initiative |


## 2\. Evaluation Mission and Test Motivation

### 2.1 Background

The key reason we use testing is, that we want to ensure, that the code we already have works fine, that the use cases work and that new changes do not break the application.

### 2.2 Evaluation Mission

Testing is a crucial phase in the development cycle. It is necessary in order to exterminate technical bugs and important functional problems and gives assurance of their absence. Behaviour driven development – which we actively live out – actually includes functional and UI testing in it's core. It helps teams to communicate and verify the requirements in use cases over different technical and functional levels (from Techie to Manager).

### 2.3 Test Motivators

Due to the high code base the project has, we have to maintain and refactor our code regularly, every time a new use case is added. To ensure that future updates do not break the application. It's also very convenient, that a lot of tests are already provided or are generated by the framework from the specification.

## 3\. Test Approach

### 3.1 Unit Testing

The primary goal of unit testing is to take the smallest piece of testable software in the application, isolate it from the remainder of the code, and determine whether it behaves exactly as you expect. Each unit is tested separately before integrating them into modules to test the interfaces between modules. Unit testing has proven its value in that a large percentage of defects are identified during its use.

In our project we use the JUnit framework for Java.

|  | Description |
| --- | --- |
| Technique Objective: | Ensure that the implemented code works properly and independently. |
| Technique: | Implement test methods using JUnit Framework and its annotations. |
| Oracles: | Console Output, Calculated Code Coverage by SonarCube |
| Required Tools: | Test Runner: IntelliJ IDEA or Gradle with SonarQube |
| Success Criteria: | All tests pass. |

### 3.2 User Interface Testing

User Interface (UI) testing verifies a user’s interaction with the software. The goal of UI testing is to ensure that the UI provides the user with the appropriate access and navigation through the functions of the target-of-test. In addition, UI testing ensures that the objects within the UI function as expected and conform to corporate or industry standards.

For our automated frontend tests we use programmable selenium webdrivers, that open a browser and simulate user input. The user actions are specified in Java test cases.

|  | Description |
| --- | --- |
| Technique Objective: | Exercise the following to observe and log standards conformance and target behavior: |
|  | - Navigation through the target-of-test reflecting business functions and requirements, including window-to-window, field-to- field, and use of access methods (tab keys, mouse movements, accelerator keys). |
|  | - Window objects and characteristics can be exercised–such as menus, size, position, state, and focus. |
| Technique: | User Interactions (mouse clicks, keyboard inputs, browser navigation) are emulated by a tool called SeleniumCucumber. After that, the tool compares the interface's actual appearance with the expected one. If a discrepancy is detected, the tool will report an error. |
| Oracles: | Test is successful if the graphical user interface has adopted a specific state. This state's occurance can be detected by automated checking for GUI elements like butons, labels, inputs... |
| Required Tools: | SeleniumCucumber, Ruby, ChromeDriver |
| Success Criteria: | All user interface tests have to be successful. |
| Special Considerations: | The user has to receive e-mails at some points of the process. These steps can not be tested. |

### 3.3 Functional Testing

|  | Description |
| --- | --- |
| Technique Objective: | Product evaluation based on user interaction. Gathering direct input on different aspects of the system by observing participants behaviour while in contact with the object of the test. |
| Techniques: |Hallway Testing: randomly selected users are asked to interact with the product. The interaction is observed by the designers in order to determine possible problems, causing complications for the user or preventing him or her from advancing. |
|  |Synchronous Remote Usability Testing: a version of usability testing, which is based on the usage of video conferencing for user observation. Is often used when the participants do not have an opportunity to be present at the specific test-location. |
| Oracles: | Test is successful if the user is able to proceed from the given start point to the required result by interacting with the system in the ways, expected by the observers |
| Required Tools: | No specific tools required |
| Success Criteria: | No "brick walls" (problems so serious, that the users cannot advance) |

### 3.4 Installation Tests

|  | Description |
| --- | --- |
| Technique Objective: | Quality assurance based on customers feedback on the installation and set up processes. |
| Techniques: | The test participants run an install program (package software) on their machines. All the possible configurations should receive an appropriate level of testing, so that the software can be released to a wide range of customers.|
| Oracles: | Test is successful if the user is able to install and run the software on his or her machine|
| Required Tools: | No specific tools required |
| Success Criteria: | Installation Certificate is issued by the user to confirm the successful installation. User did not run into fatal problems trying to install the software. |

The Installation Certificates for Skill Access Manager can be found in the table below:

| Participant |Date | Certificate |
| --- | --- | --- |
| Enrico Kaack | 07/06/17 | [Installation Certificate (Linux) ](https://github.com/Eynorey/SAM/blob/master/_uploads/installation_tests/InstallationTestCertificate-Enrico.pdf) |
| Tim Schmidt | 07/06/17| [Installation Certificate (Windows) ](https://github.com/Eynorey/SAM/blob/master/_uploads/installation_tests/InstallationTestCertificate-Tim.pdf)|

## 4\. Deliverables

### 4.1 Test Evaluation Summaries

The project has around 86 tests which are passing. Furthermore Visual Studio Team Services are used to the tests every time the master branch is pdated. The build will fail if the unit tests are failing.

![](test-summary.PNG)

### 4.2 Reporting on Test Coverage

After a successful build in Visual Studio Team Services, the resulting reports are uploaded to SonarQube. According to [SonarQube](https://sonarqube.com/dashboard?id=sam), the current test coverage is around 59%.

### 4.3 Perceived Quality Reports

The tool which is used for quality reports is also [SonarQube](https://sonarqube.com/dashboard?id=sam_new). It shows the amount of issues in the code as well as several metrics.

We focused on:

Incrementing Test Coverage
By improving this, we ensure even more that already implemented use cases don't break while developing new ones. That's because we regularly execute our unit tests during development.

Reducing the amount of Bugs
Issues raised by SonarQube are on either demonstrably wrong code, or code that is more likely not giving the intended behavior. Examples include null-pointer dereferences, memory leaks, and logic errors.

Reducing the amount of Code Smells
“Smelly” code does (probably) what it should, but it will be difficult to maintain. In the worst cases, it will be so confusing that maintainers can inadvertently introduce bugs. Examples include duplicated code, uncovered code by unit tests and too complex code.

The tool says that some of our methods require too many paramters, but we decided not to change this because we don't this is an issue.

SonarQube suggested four other code smells, where it says that we should make some class attributes transient.

We decided not to follow this recommendation as it breaks our code; so SonarQube falsely marked this as a bug.

## 5\. Testing Workflow

Every developer can run tests out of his or her IDE manually. To check if the developed code is compatible with the existing code.

Furthermore, Unit Tests are executed every time when the master branch is updated. This is triggered by Visual Studio Team Services:

![](CI.PNG)

## 6\. Environmental Needs

### Base Software Elements in the Test Environment

| Software Element Name | Type and Other Notes |
| --- | --- |
| JUnit | Unit Testing library |
| IntelliJ IDEA | Local Test Runner / IDE |
| Chrome Webdriver | Local UI Test Runner; OS: PC, Linux and MAC; Browser: Chrome |
| Central Unit Test Runner | Visual Studio Team Services |

### Productivity and Support Tools

| Tool Category or Type | Tool Brand Name |
| --- | --- |
| Test Management | IntelliJ IDEA |
| Test Coverage Monitor or Profiler | [SonarQube](https://sonarqube.com/dashboard?id=sam) |
| Project Management | Visual Studio Team Services |
| Hoster | Azure |
| Metrics tool | [SonarQube](https://sonarqube.com/dashboard?id=sam) |

## 7\. Responsibilities, Staffing and Training Needs

Responsible for all testing topics, including User Interface and Unit test, is Dominik Wunderlich. Other team members might have to be informed about the new Test Plan.

## 8\. Milestones

We want to keep over 50% code coverage.

## 9\. Risks

| Risk | Mitigation Strategy | Contingency (Risk is realized) |
| --- | --- | --- |
| Dependency Injection impedes isolated Unit Testing | Abstract into new unit | transfer method into new scope |
| Unit test cannot be applied because of framework structure | Pay attention to feature and code linkage | Test via Integration test |